* Notes
** Introduction - On Running Software
   - "Let it crash" (also called mildly "let it fail") - it turns out that 131
     out of 132 bugs are transient bugs (they’re non-deterministic and go away
     when you look at them, and trying again may solve the problem entirely),
     according to *Jim Gray* in *Why Do Computers Stop and What Can Be Done
     About It*?
   - Interesting analogy to immunology system (Erlang) and extreme care about
     outside hygiene and do not letting germs get into (other languages). Both
     are extremely important, but only good immune system is a long term
     solution for varying and - more important - evolving environment.
   - More interesting part is when you can diagnose running system, by observing
     it (introspection and tracing) and be a "doctor" in the run-time.
** Part I - Writing Applications
*** How to dive into a Code Base?
    - Distinction between *library* and *regular* applications. But they're
      sharing the same OTP structure. Good starting point is either
      *src/<AppName>.app.src* or *ebin/<AppName>.app* (generated or
      handcrafted).
    - Potential entry point will be either wrapper module with the same name or
      *application* behavior for *regular application*.
    - Releases are not different - it is mostly a packaged version of your app
      generated with use of `systools` or `reltool` (sometimes with something
      more modern like `relx`).
*** Building Open Source Erlang Software
    - *Mean Times Between Failures* (MTBF) of systems handling transient bugs
      being better by a *factor of 4* when doing this.
    - One very important part of Erlang supervisors and their supervision trees
      is that their start phases are synchronous. Each OTP process has the
      potential to prevent its siblings and cousins from booting. If the process
      dies, it’s retried again, and again, until it works, or fails too often.
    - No matter what, a sequence of failures is not a death sentence for the
      node. Once a system has been divided into various OTP applications, it
      becomes possible to choose which applications are vital or not to the
      node. Each OTP application can be started in 3 ways: *temporary* (even
      critical failure causes no harm for node), transient (normal exit causes
      no harm to node), permanent (any exit takes down the node as well), either
      by doing it manually in `application:start(Name, Type)`, or in the
      configuration file for your release.
*** Planning for Overload
    - It is often called - how to you avoid work when you are over of your
      capabilities. In Erlang systems, node can be killed by exhausting memory,
      because of built-up message queues. Often investigating true bottleneck is
      more expensive that it look at the first sight. Even if you will increase
      pipe radius, and build bigger pipelines, there is a lot of thing out of
      your control, which are have "smaller radius" and won't accept increased
      flow.
    - You can either do *load-shedding* (drop messages on the floor) or
      introduce *back-pressure* (do not accept more messages and inform producer
      about that fact, deliberately slow down of your users, providing to them a
      bad UX, but you know - availability is more important).
      - In case of *back-pressure* it loves to creep out from the core of your
        system down through the edges, especially in case when you have a
        sequential or lock-envy operation. Because it means that other
        asynchronous operations can built up heavily when those sequential one
        is serving previous requests. And one more thing - waiting for unbounded
        time (aka no timeouts) is silly. Why not to wait 30 years? Ah this is
        silly, and how come infinity is not? ;)
        - Pat Helland, *Idempotence is not a medical condition* (2012)
        - *Safety Valve* or *circuit breaker* is also a very nice and interesting
          pattern for that. It is also very important to communicate
          *back-pressure* mechanism violation clearly and least painfully for the
          end user as you can.
      - Another strategy is to drop traffic. Randomly, with priorities, batching
        together similar things etc. You name it. The main purpose is to drop
        the messages in order to live and not crash. Also various data
        structures like queue buffers, stack buffers, time-based buffers and
        ring buffers will be helpful for you.
        - BTW. Erlang lists are *stacks*, internally it is much more complicated
          but from behavioral point of view - it is a stack, built on top of
          single-linked list.
    - Common Erlang overload issues?
      - `error_logger` explosion - default implementation from OTP do not expect
        a lot and big messages to be dumped on it. In really high traffic
        systems couple minutes can cause a OOM issue. Use other
        implementations - e.g. one delivered with `lager` library.
      - Too eager and too fast retry can cause message queue to built-up and
        explode the node as well. Especially when you're handling blocking and
        locking operations (e.g. I/O - like listening on socket).
      - Unexpected messages and building message queue app can also cause the
        node failure.
** Part II - Diagnosing Applications
*** Connecting to Remote Nodes
    - 4 ways for connecting to Erlang shell.
      - Direct connection to node.
        - Almost any operation is unsafe here, because you're doing it on actual
          node.
        - You can spawn another node then connect via JCL mode, but do not
          invoke commands like `q().` or `halt().`. In that case use always
          CTRL + G (^G) and `q`.
      - Remote shell connection (from *normal* or *hidden* node, to avoid fully
        connected mesh afterwards).
        - JCL mode is also available, but it is safer - you'll shutdown remote
          node, instead the proper one. But only via JCL, not function mentioned
          above.
      - SSH connection - it has built-in SSH client and server, you need to
        generate keys and configure it properly (2 commands).
        - Almost any operation is unsafe here, because you're operating on
          actual node.
      - Named pipe (`run_erl` and `to_erl` commands), it is about redirecting
        standard descriptors to a named UNIX pipe. Keep in mind that it will do
        `fsync` calls when saving `STDOUT` stuff to a log file.
    - Erlang REPL is not exactly a REPL, it is something with REPL
      characteristic. But it is completely independent and parallel to the rest.
      - It is a client-server like infrastructure, with complete independence.
        You can safely connect as many sessions as you want. It is built like
        that because, VM does not need interpreter to run BEAM files, so it is
        sometimes an unnecessary dependency to have on production.
*** Run-time Metrics
    - Introspection and metrics, a truckload of them available directly from
      Erlang shell. Part of them can be unsafe, because of amount of data
      they'll return.
    - `erlang:process_info/1` is your friend at most cases, but `recon` contains
      safer version of that function, which is also more convenient because it
      groups the parameters and has help message, which describes each one in
      details.
    - Full introspection is available for normal processes, but it is even more
      convenient for OTP-like processes. They've got additional facilities in
      `sys` module - exposing internal state, FSM statuses etc. is a built-in
      functionality in OTP behaviors. That is a reason why you should use
      `proc_lib` for your custom processes, because it will provide a skeleton
      for handling those `sys` messages and many of those common facilities for
      you.
    - Not only processes, ETS, GC metrics - but also CPU, schedulers, ports are
      available for full introspection. Any detail inside VM is available for
      you. And you've got even more powerful facility called `tracing` for
      debugging stuff in real-time.
      - But it is helpful when you have exact entry points and functions which
        you can trace. Then you can do it even on production, without affecting
        the performance.
*** Reading Crash Dumps
    - Erlang crash dump is a well defined and document way of saving last breath
      of the VM. Library `recon` contains a lot of helpful tools for parsing it
      and extracting message queue details (e.g. script `queue_fun.awk` for
      determining which process has a long message queue during the crash).
    - Most probable reason why you'll see a crash dump is exhausting memory (by
      atom table, memory fragmentation, a lot of unconsumed messages in one
      process, overloading one process - a bottleneck).
    - Other reason is that one of your applications restarted too many times and
      whole node shutdown. Also when you'll hit any kind of `ulimit` (e.g. too
      many opened file descriptors)
    - Besides that it will happen when you'll deal with NIFs - and that's really
      bad stuff, because it means that sometimes you're so deep inside VM that
      you'll be on your own.
*** Memory Leaks
    - Besides programmers bug, crossing the `ulimit`s the most often cause of
      bringing down whole machine is to overrun memory by some kind of leak.
      - It can be a simple to figure out situation (e.g. binary leaks, atom
        table limit hit) or something really complicated (e.g. memory
        fragmentation).
      - Don't create new atom from source that comes from user or 3rd party
        systems, `erlang:list_to_existing_atom/1` is your friend. Check if your
        SAX parser is safe (`xmerl` is not entirely safe). Random names
        generation, which will be associated for nodes as well could be a case.
      - In case of `code` related memory, it can be related either with
        unauthorized access to your system, not garbage collected code after hot
        code reload or HiPE compilation, which will not be garbage collected,
        because it is a native code.
      - ETS is never GCed, you can reclaim memory only by removing records or
        deleting table. You can use undocumented `ets:i().` for investigating
        numbers, if that is not a case you need to either shard data across
        multiple servers or compress your table to buy some time.
      - For a process memory, you can either leak processes itself, the easiest
        way to find leaked processes is to seek for not linked processes.
        - `recon:proc_count(memory, 3).` shows the top 3 processes which have
          the biggest memory usage.
        - You can observe GC, which is another step to figure out problems with
          memory, and it can indicate that spikes, in for allocations are
          happening in VM. When memory is a scarce resource, this can be a
          problem. Tracking GC pauses, can be done with command invoked form
          shell: `erlang:system_monitor(self(), [{long_gc, 500}]).`, that will
          setup system monitor which will report GC pauses longer than 500 ms to
          the shell.
      - If it is not a memory fragmentation, binary leak, or neither of above
        probably it can be something with misbehaving NIFs, C driver or even VM
        is leaking.
      - For binaries leak, it will affect binaries larger than 64 B, because
        those are reference counted, and not included in process heap.
        - `recon:proc_count(binary_memory, 3).` returns 3 top most processes
          that have most size of allocated binaries.
        - `recon:bin_leak(5).` returns 5 top most processes that freed the most
          of binaries after forcing complete GC for whole system.
        - For fixing that you can suspend the process, spin-off the one time
          processes for handling binaries, slice the binaries and store only
          smaller chunk (size < 64 B). Or as last resort you can call GC at
          given intervals manually (icky) or stop using binaries (how?).
          - Beware middleman processes that are routing binaries, they're often
            the case. Pass binaries directly to the process that will handle
            them (e.g. data from socket).
      - For memory fragmentation often Erlang VM will report significantly
        smaller memory usage then system metrics are reporting. Also for a huge
        allocation, after GC not all of that memory will be freed. But keep in
        mind that it affects long living systems that have uptime around weeks
        and months.
        - `recon_alloc` is a module dedicated to such issues.
          - `recon_alloc:memory(usage).` call will report lower percentage than
            90-80% for a memory fragmentation (it returns percentage of memory
            actually allocated by Erlang VM for your system and processes vs.
            memory obtained from OS by Erlang VM).
          - if additionally `recon_alloc:memory(allocated).` matches what OS
            reports it means that very probably you have memory leak from Erlang
            terms or memory fragmentation.
    - How the memory in Erlang VM is structured?
      - You have got hierarchy, with two layers - two main allocators
        (`sys_alloc` which uses `malloc`, `mseg_alloc` which uses `mmap`) and 9
        different sub-allocators that are handling different types of
        allocations. Each one has different strategy of allocation - best fit, a
        fit, best fit for address etc. The only way to fix the memory
        fragmentation is to tweak those settings and verify if it helps in the
        long run, and because it happens after a long time for a few cases, it
        will take a long time to determine if it is fixed or not.
*** CPU and Scheduler Hogs
    - It is generally difficult to properly analyze the CPU usage of an Erlang
      node to pin problems to a specific piece of code. With everything
      concurrent and in a virtual machine, there is no guarantee you will find
      out if a specific process, driver, your own Erlang code, NIFs you may have
      installed, or some third-party library is eating up all your processing
      power.
    - The existing approaches are often limited to profiling and
      reduction-counting if it’s in your code, and to monitoring the scheduler’s
      work if it might be anywhere else (but also your code).
    - Two ways of detecting:
      - Profiling (it uses tracing, beware!):
        - eprof - oldest, deliver percentage usage.
        - fprof - more powerful, very detailed.
        - eflame - newest one, generates flame graphs.
      - `recon:proc_window(reductions, 3, 500).`
        - It works on reductions, higher reductions count is mostly related with
          higher CPU usage. Second argument means top N, third argument means
          lowest possible threshold.
      - You've got available also `erlang:system_monitor/1` facility with
        argument `long_schedule` - it can confirm that your problems have nature
        related with NIFs and internals, something that break the schedulers
        (often long running NIFs are messing up with schedulers and their
        reductions count).
        - Also beware of long GC pauses, because their will affect scheduling.
      - If until then, nothing shown an obvious thing you should point into
        suspended ports, by investigating results of system monitor from
        `busy_port` and `busy_dist_port` (when you're using Distributed Erlang).
        - If that's the culprit you can pass `nosuspend` to
          `erlang:port_command/3` method as an option and the same option for
          `erlang:send/3` when communicating with remote node.
*** Tracing
    - Forget about debuggers in Erlang world, usage is really limited.
      - How you'll do breaks in concurrent world of so many processes?
        - You can't and deal with it. ;)
    - Five modules:
      - `sys` which provides a lot of information for OTP compliant processes.
        However it is limited to the development mode, due to lack of
        redirection to remote shell and throttling.
      - `dbg` tracing interface, with clunky API, but still really powerful. And
        you have to know what are you doing, because it can kill node under 2
        seconds.
      - *Tracing BIFs* in large application they'll be hard to use, due to raw
        usage, but they might be useful in development mode.
      - `redbug` is a production safe library, part of `eper` suite. It has rate
        limiter, nice interface.
      - `recon_trace` is available with `recon` - the goal was to provide
        `redbug` but without additional dependencies from `eper` suite. It can
        trace only function calls, not messages as `redbug` and API for rate
        limiting and usage is different.
    - Start with a very limited scope, and then progressively broaden it when
      you made progress.
** Conclusion
   - "I hope that after reading this text, the next time stuff goes bad, it
     won’t go too bad."
   - Even the cleanest systems, that are in the wild, in maintenance mode have
     some edges and corners, and during that time everyone have to face the
     errors.
   - "Even the most solid bridges need to be repainted all the time in order
     avoid corrosion to the point of their collapse."
